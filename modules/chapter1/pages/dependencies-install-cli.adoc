= Installing Dependencies Using the CLI

== Installation of Red{nbsp}Hat Openshift Data Science Dependencies

As described in the xref::dependencies-install-web-console.adoc[Installing Dependencies Using the Web Console] section, some components of *Red{nbsp}Hat Openshift Data Science* require additional operators to be installed.  

In general not installing dependencies before the *Red{nbsp}Hat Openshift Data Science* does not impact the installation process itself, however it may impact initialization of the components that depend on them. Hence it's better to install the dependencies beforehand.

https://www.redhat.com/en/technologies/cloud-computing/openshift/pipelines[Red{nbsp}Hat Openshift Pipelines Operator]::
The *Red Hat Openshift Pipelines Operator* is required if you want to install the *Data Science Pipelines* component.
https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html[NVIDIA GPU Operator]::
The *NVIDIA GPU Operator* is required for GPU support in *Red Hat Openshift Data Science*.
https://docs.openshift.com/container-platform/4.13/hardware_enablement/psap-node-feature-discovery-operator.html[Node Feature Discovery Operator]::
The *Node Feature Discovery Operator* is a prerequisite for the *NVIDIA GPU Operator*.


This section will discuss the process for installing the dependent operators using the cli.

== Installation of Data Science Pipelines Dependencies

The Data Science Pipelines component utilizes *Red{nbsp}Hat Openshift Pipelines* as an execution engine for all pipeline runs, and is required to be installed to take advantage of the Data Science Pipelines component.

The following section discusses installing the *Red{nbsp}Hat Openshift Pipelines* operator.

=== Demo: Installation of the *Red{nbsp}Hat Openshift Pipelines* Operator
[WARNING]
If you have already installed the *Red{nbsp}Hat Openshift Data Science* operator during the previous demonstration, you have to uninstall it. Follow the xref:uninstalling-rhods.adoc#demo-rhods[Uninstalling Red{nbsp}Hat Openshift Data Science] demo to uninstall the *Red{nbsp}Hat Openshift Data Science* operator first and xref:uninstalling-rhods.adoc#demo-pipelines[Uninstalling Red{nbsp}Hat Openshift Pipelines] to uninstall *Red{nbsp}Hat Openshift Pipelines* first.

. Log in as a user with the _cluster-admin_ role assigned.
+
[subs=+quotes]
----
$ *oc login -u admin -p _password_ https://api...:6443*
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
----

. The Pipelines operator's default namespace is _openshift-operators_, so neither the namespace nor operator group resources must be created. Create only the *Subscription* resource to start the installation. 
+
--
[subs=+quotes]
----
$ *cat <<EOF > pipelines-subs.yaml*
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-pipelines-operator-rh
  namespace: openshift-operators
spec:
  channel: latest <1>
  installPlanApproval: Automatic <2>
  name: openshift-pipelines-operator-rh
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

$ *oc create -f pipelines-subs.yaml*
----
<1> The update channel to install the operator from. To find all available channels see the xref:olm-overview.adoc#findchannel[Finding Update Channels] section.
<2> In case the *installPlanApproval* is set to *Manual*, approve the installation first to start it. Refer to the xref:olm-overview.adoc#manual_approval[Approving Installation Manually] section for more information.
--
. You may check the installation progress using the operator's status information. For more information see the xref:olm-overview.adoc#installprogress[Checking Installation Progress] section.

== Installation of GPU Dependencies

*Red{nbsp}Hat Openshift Data Science* makes it easy to expose GPUs to end users to help accelerate training and serving machine learning models.

Currently, *Red{nbsp}Hat Openshift Data Science* supports accelerated compute with NVIDIA GPUs using the *NVIDIA GPU Operator* which relies on the *Node Feature Discovery* operator as a dependency.

The following section will discuss the installation and a basic configuration of both *NVIDIA GPU Operator* and the *Node Feature Discovery* operator.

TIP: To view the list of GPU models supported by the *NVIDIA GPU Operator* refer to the https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems[Supported NVIDIA GPUs and Systems] docs.

=== Demo: Installation of the *Node Feature Discovery* operator

. Log in as a user with the _cluster-admin_ role assigned. 
+
[subs=+quotes]
----
$ *oc login -u admin -p _password_ https://api...:6443*
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
----

. Create the openshift-nfd *Namespace* and *Operator Group*.
+
--
[subs=+quotes]
----
$ *cat <<EOF > nfd-ns.yaml*
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-nfd
  labels:
    openshift.io/cluster-monitoring: 'true'
EOF

$ *oc create -f nfd-ns.yaml*
namespace/openshift-nfd created

$ *cat <<EOF > nfd-og.yaml*
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  generateName: openshift-nfd-
  name: openshift-nfd
  namespace: openshift-nfd
spec:
  targetNamespaces:
  - openshift-nfd
EOF

$ *oc create -f nfd-og.yaml*
operatorgroup.operators.coreos.com/openshift-nfd created
----
--
. Now create the  operator's subscription to start the installation.
+
--
[subs=+quotes]
----
$ *cat <<EOF > nfd-sub.yaml*
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nfd
  namespace: openshift-nfd
spec:
  channel: "stable"
  installPlanApproval: Automatic
  name: nfd
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF

$ *oc create -f nfd-sub.yaml*
subscription.operators.coreos.com/nfd created
----

. Check the installation progress using the operator's status information. For more information see the xref:olm-overview.adoc#installprogress[Checking Installation Progress] section.
+
[subs=+quotes]
----
*$ oc get olm | grep ^clusterserviceversion | grep nfd*

clusterserviceversion... ... Node Feature Discovery Operator 4.12.0-202311021630  Succeeded  <1>
----
<1> Make sure that the status is *Succeeded*

. Finally create the NodeFeatureDiscovery object.
+
----
cat <<EOF > nfd.yaml
kind: NodeFeatureDiscovery
apiVersion: nfd.openshift.io/v1
metadata:
  name: nfd-instance
  namespace: openshift-nfd
spec:
  customConfig:
    configData: |
      #    - name: "more.kernel.features"
      #      matchOn:
      #      - loadedKMod: ["example_kmod3"]
      #    - name: "more.features.by.nodename"
      #      value: customValue
      #      matchOn:
      #      - nodename: ["special-.*-node-.*"]
  operand:
    image: >-
      registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:latest
    servicePort: 12000
  workerConfig:
    configData: |
      core:
      #  labelWhiteList:
      #  noPublish: false
        sleepInterval: 60s
      #  sources: [all]
      #  klog:
      #    addDirHeader: false
      #    alsologtostderr: false
      #    logBacktraceAt:
      #    logtostderr: true
      #    skipHeaders: false
      #    stderrthreshold: 2
      #    v: 0
      #    vmodule:
      ##   NOTE: the following options are not dynamically run-time 
      ##          configurable and require a nfd-worker restart to take effect
      ##          after being changed
      #    logDir:
      #    logFile:
      #    logFileMaxSize: 1800
      #    skipLogHeaders: false
      sources:
      #  cpu:
      #    cpuid:
      ##     NOTE: whitelist has priority over blacklist
      #      attributeBlacklist:
      #        - "BMI1"
      #        - "BMI2"
      #        - "CLMUL"
      #        - "CMOV"
      #        - "CX16"
      #        - "ERMS"
      #        - "F16C"
      #        - "HTT"
      #        - "LZCNT"
      #        - "MMX"
      #        - "MMXEXT"
      #        - "NX"
      #        - "POPCNT"
      #        - "RDRAND"
      #        - "RDSEED"
      #        - "RDTSCP"
      #        - "SGX"
      #        - "SSE"
      #        - "SSE2"
      #        - "SSE3"
      #        - "SSE4.1"
      #        - "SSE4.2"
      #        - "SSSE3"
      #      attributeWhitelist:
      #  kernel:
      #    kconfigFile: "/path/to/kconfig"
      #    configOpts:
      #      - "NO_HZ"
      #      - "X86"
      #      - "DMI"
        pci:
          deviceClassWhitelist:
            - "0200"
            - "03"
            - "12"
          deviceLabelFields:
      #      - "class"
            - "vendor"
      #      - "device"
      #      - "subsystem_vendor"
      #      - "subsystem_device"
      #  usb:
      #    deviceClassWhitelist:
      #      - "0e"
      #      - "ef"
      #      - "fe"
      #      - "ff"
      #    deviceLabelFields:
      #      - "class"
      #      - "vendor"
      #      - "device"
      #  custom:
      #    - name: "my.kernel.feature"
      #      matchOn:
      #        - loadedKMod: ["example_kmod1", "example_kmod2"]
      #    - name: "my.pci.feature"
      #      matchOn:
      #        - pciId:
      #            class: ["0200"]
      #            vendor: ["15b3"]
      #            device: ["1014", "1017"]
      #        - pciId :
      #            vendor: ["8086"]
      #            device: ["1000", "1100"]
      #    - name: "my.usb.feature"
      #      matchOn:
      #        - usbId:
      #          class: ["ff"]
      #          vendor: ["03e7"]
      #          device: ["2485"]
      #        - usbId:
      #          class: ["fe"]
      #          vendor: ["1a6e"]
      #          device: ["089a"]
      #    - name: "my.combined.feature"
      #      matchOn:
      #        - pciId:
      #            vendor: ["15b3"]
      #            device: ["1014", "1017"]
      #          loadedKMod : ["vendor_kmod1", "vendor_kmod2"]
EOF

oc create -f nfd.yaml
----

. Now that *Node Feature Discovery* has been configured, you can check the status of the nfd-worker pods.
+
[subs=+quotes]
----
$ *oc get pods -n openshift-nfd*
NAME                                      READY   STATUS    RESTARTS   AGE
nfd-controller-manager-777944c5f6-7pjx8   2/2     Running   0          3h48m
nfd-master-58d9f8855f-j2hcc               1/1     Running   0          3h38m
nfd-worker-99p7t                          1/1     Running   0          3h38m
nfd-worker-bwfxv                          1/1     Running   0          3h38m
nfd-worker-g5cm8                          1/1     Running   0          44m
nfd-worker-km7c5                          1/1     Running   0          3h38m
----

All pods should report as Ready.

=== Demo: Installation of the *NVIDIA GPU Operator*

. Log in as a user with the _cluster-admin_ role assigned. 
+
[subs=+quotes]
----
$ *oc login -u admin -p _password_ https://api...:6443*
Login successful.

You have access to 74 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
----
--
. Create the nvidia-gpu-operator *Namespace* and *Operator Group*.
+
--
[subs=+quotes]
----
$ *cat <<EOF > gpu-ns.yaml*
apiVersion: v1
kind: Namespace
metadata:
  name: nvidia-gpu-operator
  labels:
    openshift.io/cluster-monitoring: 'true'
EOF

$ *oc create -f gpu-ns.yaml*
namespace/nvidia-gpu-operator created

$ *cat <<EOF > gpu-og.yaml*
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: nvidia-gpu-operator-group
  namespace: nvidia-gpu-operator
spec:
 targetNamespaces:
 - nvidia-gpu-operator
EOF

$ *oc create -f gpu-og.yaml*
operatorgroup.operators.coreos.com/nvidia-gpu-operator-group created
----
--
. Next, get the default channel for the operator.
+
--
[subs=+quotes]
----
$ *CHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}')*
----
--
. Now create the  operator's subscription to start the installation.
+
--
[subs=+quotes]
----
$ *cat <<EOF > gpu-sub.yaml*
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: gpu-operator-certified
  namespace: nvidia-gpu-operator
spec:
  channel: "$(echo ${CHANNEL})"
  installPlanApproval: Manual
  name: gpu-operator-certified
  source: certified-operators
  sourceNamespace: openshift-marketplace
  startingCSV: "gpu-operator-certified.v22.9.0"
EOF

$ *oc create -f gpu-sub.yaml*
subscription.operators.coreos.com/gpu-operator-certified created
----

. Check the installation progress using the operator's status information. For more information see the xref:olm-overview.adoc#installprogress[Checking Installation Progress] section.
+
[subs=+quotes]
----
*$ oc get olm -n nvidia-gpu-operator | grep ^clusterserviceversion | grep gpu*

clusterserviceversion... ... gpu-operator-certified.v23.6.1  Succeeded  <1>
----
<1> Make sure that the status is *Succeeded*

. Finally create the ClusterPolicy object.
+
----
cat <<EOF > clusterpolicy.yaml
kind: ClusterPolicy
apiVersion: nvidia.com/v1
metadata:
  name: gpu-cluster-policy
spec:
  operator:
    defaultRuntime: crio
    use_ocp_driver_toolkit: true
    initContainer: {}
  sandboxWorkloads:
    enabled: false
    defaultWorkload: container
  driver:
    enabled: true
    useNvidiaDriverCRD: false
    upgradePolicy:
      autoUpgrade: true
      drain:
        deleteEmptyDir: false
        enable: false
        force: false
        timeoutSeconds: 300
      maxParallelUpgrades: 1
      maxUnavailable: 25%
      podDeletion:
        deleteEmptyDir: false
        force: false
        timeoutSeconds: 300
      waitForCompletion:
        timeoutSeconds: 0
    repoConfig:
      configMapName: ''
    certConfig:
      name: ''
    licensingConfig:
      nlsEnabled: true
      configMapName: ''
    virtualTopology:
      config: ''
    kernelModuleConfig:
      name: ''
  dcgmExporter:
    enabled: true
    config:
      name: ''
    serviceMonitor:
      enabled: true
  dcgm:
    enabled: true
  daemonsets:
    updateStrategy: RollingUpdate
    rollingUpdate:
      maxUnavailable: '1'
  devicePlugin:
    enabled: true
    config:
      name: ''
      default: ''
  gfd:
    enabled: true
  migManager:
    enabled: true
  nodeStatusExporter:
    enabled: true
  mig:
    strategy: single
  toolkit:
    enabled: true
  validator:
    plugin:
      env:
        - name: WITH_WORKLOAD
          value: 'false'
  vgpuManager:
    enabled: false
  vgpuDeviceManager:
    enabled: true
  sandboxDevicePlugin:
    enabled: true
  vfioManager:
    enabled: true
  gds:
    enabled: false

EOF

oc create -f clusterpolicy.yaml
----
